{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes sentiment analysis of a corpus with NLTK. It's part of the [The Art of Literary Text Analysis](ArtOfLiteraryTextAnalysis.ipynb) (and assumes that you've already worked through previous notebooks – see the table of contents). In this notebook we'll look in particular at:\n",
    "\n",
    "* [Creating a sonnets corpus](#Create-a-Sonnets-Corpus)\n",
    "* [Using a plain text corpus reader](#Plain-Text-Corpus-Reader)\n",
    "* [Introducing sentiment analysis](#Introduction-to-Sentiment-Analysis)\n",
    "* [Doing sentiment analysis with word lists](#Word-List-Sentiment-Analysis)\n",
    "* [Doing sentiment analysis with WordNet](#Sentiment-Analysis-with-WordNet)\n",
    "* [Doing sentiment analysis of Shakespeare's sonnets](#Sentiment-Analysis-of-Shakespeare's-Sonnets)\n",
    "* [Outputting HTML](#Outputting-HTML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Sonnets Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to start by downloading [Shakespeare's sonnets from Gutenberg](http://www.gutenberg.org/cache/epub/1041/pg1041.txt). The ususal provisos apply regarding web-based URLs and particularly ones from Gutenberg: content can be inaccessible (moved, blocked, etc.), and it's best to limit the number of times you make the actual request, so we'll isolate the reading in its own cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122774"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "sonnetsUrl = \"http://www.gutenberg.org/cache/epub/1041/pg1041.txt\"\n",
    "sonnetsString = urllib.request.urlopen(sonnetsUrl).read().decode()\n",
    "len(sonnetsString) # let's make sure we have a lot of characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Gutenberg text includes additional elements like a header and footer that we don't want to include in our analysis, so just as we did in [Getting Texts](GettingTexts.ipynb), we need to isolate the main body of text. To do that we can [look more closely at the text](http://www.gutenberg.org/cache/epub/1041/pg1041.txt) and see that the first sonnet (and each of the subsequent ones) starts with a roman numeral (\"I\") and that the text ends with \"End of Project Gutenberg's\", so let's create a filtered sonnets string by finding the locations of our two marker strings and keeping only the text in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  I\r\n",
      "\r\n",
      "  From fairest creatures we desire increase,\r\n",
      "  That thereby beauty' … d this by that I prove,\r\n",
      "    Love's fire heats water, water cools not love.\n"
     ]
    }
   ],
   "source": [
    "filteredSonnetsStart = sonnetsString.find(\"  I\\r\\n\") # title of first sonnet\n",
    "filteredSonnetsEnd = sonnetsString.find(\"End of Project Gutenberg's\") # end of sonnets\n",
    "filteredSonnetsString = sonnetsString[filteredSonnetsStart:filteredSonnetsEnd].rstrip() # strip spaces from the end (right)\n",
    "print(filteredSonnetsString[:75] + ' … ' + filteredSonnetsString[-75:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In looking at the sonnets we see that they all follow the same format of a roman numeral and then a block of text. That's a pattern that's fairly easy to define with regular expressions, we can look for the following:\n",
    "\n",
    "* Two spaces (all the text is indented): \"  \"\n",
    "* One or more uppercase characters that represent the roman numerals (I, IV, XI, etc.) ```[A-Z]+```\n",
    "  * We can do this by using a character range that says any character between A and Z or ```[A-Z]+```\n",
    "  * Alternatively, we could enumerate a list of relevant characters ```[IVXLC]+```\n",
    "  * The plus sign at the end specifies that we should match one or more instances of what preceeds\n",
    "* Two Windows-style carriage returns and newlines: \"\\r\\n\\r\\n\"\n",
    "\n",
    "Putting that together we get: ```\"  [A-Z]+\\r\\n\\r\\n\"``` and we can use this to split our filtered sonnets into chunks based on the roman numeral separators. This is a bit like splitting the cells in a row of comma-separated values, except that instead of knowing exactly which delimeter character to use (like a comma), we're defining a flexible delimeter that can contain types of characters. We use [re.split()](https://docs.python.org/3/library/re.html#re.split) to split the original string into a list of strings (one for each sonnet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  From fairest creatures we desire increase,\r\n",
      "  That thereby beauty's rose might never die,\r\n",
      "  But as the riper should by time decease,\r\n",
      "  His tender heir might bear his memory:\r\n",
      "  But thou contracted to thine own bright eyes,\r\n",
      "  Feed'st thy light's flame with self-substantial fuel,\r\n",
      "  Making a famine where abundance lies,\r\n",
      "  Thy self thy foe, to thy sweet self too cruel:\r\n",
      "  Thou that art now the world's fresh ornament,\r\n",
      "  And only herald to the gaudy spring,\r\n",
      "  Within thine own bud buriest thy content,\r\n",
      "  And tender churl mak'st waste in niggarding:\r\n",
      "    Pity the world, or else this glutton be,\r\n",
      "    To eat the world's due, by the grave and thee.\r\n",
      "\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "sonnetsList = re.split(\"  [A-Z]+\\r\\n\\r\\n\", filteredSonnetsString)\n",
    "print(sonnetsList[1]) # sonnetList[0] is empty (the text before the first roman numeral), let's look at sonnet I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of sonnets, let's write them to a file in case we want to load them later.\n",
    "\n",
    "Just as we did with [Getting Texts](GettingTexts.ipynb), we'll create a directory (if it doesn't exist already)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "sonnetsPath = 'sonnets' # this subdirectory will be relative to the current notebook\n",
    "if not os.path.exists(sonnetsPath):\n",
    "    os.makedirs(sonnetsPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's enumerate through each sonnet (we [enumerate](https://docs.python.org/3/library/functions.html#enumerate) so that we can get the index of each item in our list), and for each sonnet we'll write it into a new file. We won't use the roman numerals for the file names, but we'll use [str.zfill()](https://docs.python.org/3/library/stdtypes.html?highlight=zfill#str.zfill) to get zero-padded strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "print(str(1).zfill(3)) # convert the number 1 to a string and prepend zeros as needed to get three characters\n",
    "print(str(150).zfill(3)) # convert the number 150 to a string and prepend zeros as needed to get three characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, sonnet in enumerate(sonnetsList): # loop through our list as enumeration to get index\n",
    "    if len(sonnet.strip()) > 0: # make sure we have text, not empty after stripping out whitespace\n",
    "        filename = str(index).zfill(3)+\".txt\" # create filename from index\n",
    "        pathname = os.path.join(sonnetsPath, filename) # directory name and filenamee\n",
    "        f = open(pathname, \"w\")\n",
    "        f.write(sonnet.rstrip()) # write out our sonnet into the file (removing whitespace at the end/right)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we navigate to the directory where our notebook is we should now have a subdirectory called \"sonnets\" with 154 text files, one per sonnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plain Text Corpus Reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already seen how we can work with many files in a directory using a corpus reader. For instance, we can look at the Gutenberg corpus and ask how many files there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.corpus.gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't do the same thing directly with our sonnetsPath directory because it's not a corpus.\n",
    "\n",
    "```python\n",
    "sonnetsPath.fileids() # this won't work, it's not a corpus\n",
    "nltk.corpus.sonnetsPath.fileids() # neither is this (nltk.corpus doesn't have a sonnetsPath module)```\n",
    "\n",
    "What we can do is to create a new corpus using the [PlaintextCorpusReader](http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.plaintext.PlaintextCorpusReader), giving it the path of our directory and telling it to consider all texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "sonnetsCorpus = PlaintextCorpusReader(sonnetsPath, '.*txt')\n",
    "len(sonnetsCorpus.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see how many words are in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this corpus has\n",
      "   154 files\n",
      "   21,717 tokens\n",
      "   21,717 words\n",
      "   3,358 unique word types\n"
     ]
    }
   ],
   "source": [
    "def corpus_summary(corpus):\n",
    "    print(\"this corpus has\")\n",
    "    print(\"  \", '{:,}'.format(len(sonnetsCorpus.fileids())), \"files\")\n",
    "    tokens = corpus.words() # this includes *all* words in all files\n",
    "    print(\"  \", '{:,}'.format(len(tokens)), \"tokens\")\n",
    "    words = [word for word in tokens if word[0].isalpha()]\n",
    "    print(\"  \", '{:,}'.format(len(tokens)), \"words\")\n",
    "    print(\"  \", '{:,}'.format(len(set(tokens))), \"unique word types\")\n",
    "\n",
    "corpus_summary(sonnetsCorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Sentiment analysis](http://en.wikipedia.org/wiki/Sentiment_analysis) is a general term used in text mining to refer to the process of trying to automatically determine the mood or opinion of texts. For example, we might be interested in reading through a large quantity of tweets on a particular topic (issue, product, person, event, etc.) to determine if they are generally positive or negative. This is a typical task in determining polarity, or the general orientation of each text toward two poles. The poles can be positive or negative or any other binary pair (happy/sad, calm/excited, etc.).\n",
    "\n",
    "There are several ways of doing sentiment analysis and one of the most common ways is to use training sets to develop models of texts that can then be used to read and classify new texts that aren't part of the training set. For instance, NLTK has a corpus of movie reviews that have been classified by hand as either positive or negative, these could be used to classify new movie reviews.\n",
    "\n",
    "There's a high-level Python module called [TextBlob](https://textblob.readthedocs.org/en/latest/index.html) that makes use of the movie reviews corpus and that is very easy to use. Once [installed](https://textblob.readthedocs.org/en/latest/install.html), you can do something like this:\n",
    "\n",
    "```python\n",
    "from textblob import TextBlob\n",
    "text = \"Textblob is amazingly simple to use. What great fun!\"\n",
    "testimonial = TextBlob(text) # any string (such as our sonnets)\n",
    "testimonial.sentiment.polarity # this produces a positive value of 0.39166666666666666```\n",
    "\n",
    "The strength of this approach depends in large part on the training set and its relevance to the corpus you want to analyze. Analyzing Shakespeare's sonnets based on movie reviews may not be quite ideal as it can be tricky to understand and tweak a model once it's been loaded into a classifier (such as [a Naive Bayes Classifier](http://www.nltk.org/api/nltk.classify.html#nltk.classify.naivebayes.NaiveBayesClassifier)).\n",
    "\n",
    "In any case, classifiers depend on relevant training sets, and we don't always have those. We can take a simpler approach and perform sentiment analysis based on word lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word List Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some ways, sentiment analysis can be very simple: read each text, look for words that we know are positive or negative, and assign a value to each one. Let's start by defining an absurdly short and incomplete list of sentiment words, with values ranging from -1 (most negative) to 1 (most positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "short_sentiments = {\n",
    "    \"pos\": {\n",
    "        \"love\": 1,\n",
    "        \"like\": .5\n",
    "    },\n",
    "    \"neg\": {\n",
    "        \"hate\": -1,\n",
    "        \"dislike\": -.5\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go through our corpus and read every text to find out how many times each of our sentiment words appear. We'll multiply the count by the value for that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = {} # keep track of values for each document\n",
    "for fileid in sonnetsCorpus.fileids():\n",
    "    text = sonnetsCorpus.raw(fileid).lower() # read each text and convert to lower case\n",
    "    tokens = nltk.word_tokenize(text) # tokenize\n",
    "    score = 0 # keep track of the score fo this document\n",
    "    for polarity, words_dict in short_sentiments.items(): # look at our sentiments dictionary\n",
    "        for word, value in words_dict.items(): # go through each word in the list\n",
    "            score += tokens.count(word) * value # count the occurences and multiply by value (often zero)\n",
    "    documents[fileid] = score # assign the score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our fileid, score dictionary to create a FreqDist list and retrieve the most positive sonnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Take all my loves, my love, yea take them all;\r\n",
      "  What hast thou then more than thou hadst before?\r\n",
      "  No love, my love, that thou mayst true love call;\r\n",
      "  All mine was thine, before thou hadst this more.\r\n",
      "  Then, if for my love, thou my love receivest,\r\n",
      "  I cannot blame thee, for my love thou usest;\r\n",
      "  But yet be blam'd, if thou thy self deceivest\r\n",
      "  By wilful taste of what thyself refusest.\r\n",
      "  I do forgive thy robbery, gentle thief,\r\n",
      "  Although thou steal thee all my poverty:\r\n",
      "  And yet, love knows it is a greater grief\r\n",
      "  To bear love's wrong, than hate's known injury.\r\n",
      "    Lascivious grace, in whom all ill well shows,\r\n",
      "    Kill me with spites yet we must not be foes.\n"
     ]
    }
   ],
   "source": [
    "valuesFreqs = nltk.FreqDist(documents) # calcuate the frequencies\n",
    "mostFrequentFileid = valuesFreqs.max() # most positive (top value) sonnet\n",
    "print(sonnetsCorpus.raw(mostFrequentFileid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This word list approach has the great merit of being fairly easy to understand, we know which words we're dealing with (and can easily modify the list) and we can see exactly how it's working. The major weakness, of course, is that we need to define the word sentiment values in advance, which means that we need to know which sentiment words are in the corpus (or use a generic list).\n",
    "\n",
    "An alternative is to call on the sentiment values that are encoded in WordNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet includes a mechanism for determining the positive or negative value of a synset (if any values are defined). In the simplest case, we can ask for all of the sentiment values for a given word form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<good.n.01: PosScore=0.5 NegScore=0.0> benefit\n",
      "<good.n.02: PosScore=0.875 NegScore=0.0> moral excellence or admirableness\n",
      "<good.n.03: PosScore=0.625 NegScore=0.0> that which is pleasing or valuable or useful\n",
      "<commodity.n.01: PosScore=0.0 NegScore=0.0> articles of commerce\n",
      "<good.a.01: PosScore=0.75 NegScore=0.0> having desirable or positive qualities especially those suitable for a thing specified\n",
      "<good.a.03: PosScore=1.0 NegScore=0.0> morally admirable\n",
      "<well.r.01: PosScore=0.375 NegScore=0.0> (often used as a combining form) in a good or proper or satisfactory manner or to a high standard (`good' is a nonstandard dialectal variant for `well')\n",
      "<thoroughly.r.02: PosScore=0.0 NegScore=0.0> completely and absolutely (`good' is sometimes used informally for `thoroughly')\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "for senti_synset in swn.senti_synsets('good'):\n",
    "    print(senti_synset, senti_synset.synset.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, most meanings of \"good\" have a positive value, though some meanings are indicated as being neutral (_commodity.n.01_, _thoroughly.r.02_). Some meanings are more positive than others (such as _good.a.03_ \"morally admirable\").\n",
    "\n",
    "If we can guess at the part of speech we can narrow our search. Here are the possible parts of speech:\n",
    "\n",
    "* n: noun\n",
    "* a: adjective\n",
    "* v: verb\n",
    "* r: adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<good.a.01: PosScore=0.75 NegScore=0.0> having desirable or positive qualities especially those suitable for a thing specified\n",
      "<good.a.03: PosScore=1.0 NegScore=0.0> morally admirable\n"
     ]
    }
   ],
   "source": [
    "for senti_synset in swn.senti_synsets('good', 'a'): # consider only adjectives\n",
    "    print(senti_synset, senti_synset.synset.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, some senti_synsets can be both positive and negative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<wicked.a.01: PosScore=0.25 NegScore=0.625> morally bad in principle or practice\n"
     ]
    }
   ],
   "source": [
    "for senti_synset in swn.senti_synsets('wicked', 'a'): # consider only adjectives\n",
    "    print(senti_synset, senti_synset.synset.definition()) # wicked can be positive or negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine NLTK's part of speech analysis with the sentiment analysis module. As a reminder, the default NLTK parts of speech output looks something like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('good', 'JJ'),\n",
       " ('sentence', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is a good sentence.\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need a way of mapping the [Treebank](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) NLTK parts of speech to the WordNet ones.\n",
    "\n",
    "For this function we'll demonstrate the use of an option argument. We can specify a variable name as being equal to something such that if we don't include a second argument to the function call, it defaults to the value we provide.\n",
    "\n",
    "* treebank_to_wordnet_pos(treebank) # this form uses the default empty list []\n",
    "* treebank_to_wordnet_pos(treebank, [\"v\"]) # this form specifies a second argument with a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def treebank_to_wordnet_pos(treebank, skipWordNetPos=[]):\n",
    "    if \"NN\" in treebank and \"n\" not in skipWordNetPos: # singular and plural nouns (NN, NNS)\n",
    "        return \"n\"\n",
    "    elif \"JJ\" in treebank and \"a\" not in skipWordNetPos: # adjectives including comparatives and superlatives (JJ, JJR, JJS)\n",
    "        return \"a\" \n",
    "    elif \"VB\" in treebank and \"v\" not in skipWordNetPos: # verbs in various forms (VB, VBD, VBG, VBN, VBP, VBZ)\n",
    "        return \"v\"\n",
    "    elif \"RB\" in treebank and \"r\" not in skipWordNetPos: # adverbs including comparatives and superlatives (RB, RBR, RBS)\n",
    "        return \"r\"\n",
    "    # if we don't match any of these we implicitly return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using our treebank_to_wordnet_pos() lookup function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is v\n",
      "good a\n",
      "sentence n\n"
     ]
    }
   ],
   "source": [
    "for word, treebank in tagged:\n",
    "    wordnet_pos = treebank_to_wordnet_pos(treebank)\n",
    "    if wordnet_pos: # only print matches\n",
    "        print(word, wordnet_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a variant where we use the ability to skip certain parts of speech (like \"v\" for verbs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good a\n",
      "sentence n\n"
     ]
    }
   ],
   "source": [
    "for word, treebank in tagged:\n",
    "    wordnet_pos = treebank_to_wordnet_pos(treebank, [\"v\"])\n",
    "    if wordnet_pos: # only print matches\n",
    "        print(word, wordnet_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the WordNet senti_synsets for each of our word, part of speech pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "   <good.a.01: PosScore=0.75 NegScore=0.0>\n",
      "   <good.a.03: PosScore=1.0 NegScore=0.0>\n",
      "sentence\n",
      "   <sentence.n.01: PosScore=0.0 NegScore=0.0>\n",
      "   <conviction.n.02: PosScore=0.0 NegScore=0.0>\n",
      "   <prison_term.n.01: PosScore=0.0 NegScore=0.0>\n"
     ]
    }
   ],
   "source": [
    "for word, treebank in tagged:\n",
    "    wordnet_pos = treebank_to_wordnet_pos(treebank, [\"v\"])\n",
    "    if wordnet_pos: # only print matches\n",
    "        print(word)\n",
    "        for senti_synset in swn.senti_synsets(word, wordnet_pos):\n",
    "            print(\"  \", senti_synset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have to make a decision: do we take into consideration every senti_synset or just the first (and most common) meaning? When processing a larger corpus we have no way of knowing which meaning is correct. For the sake of simplicity, let's just consider the first meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentiment_score_from_tagged(token, treebank, skipWordNetPos=[]):\n",
    "    wordnet_pos = treebank_to_wordnet_pos(treebank, skipWordNetPos)\n",
    "    if wordnet_pos: # only print matches\n",
    "        senti_synsets = list(swn.senti_synsets(token, wordnet_pos))\n",
    "        if senti_synsets:\n",
    "            return senti_synsets[0].pos_score() - senti_synsets[0].neg_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And trying it out with our simple example tagged set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good 0.75\n"
     ]
    }
   ],
   "source": [
    "for word, treebank in tagged:\n",
    "    score = get_sentiment_score_from_tagged(word, treebank, [\"v\"])\n",
    "    if score:\n",
    "        print(word, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we're now able to get a sentiment score for any tagged word (that is defined by WordNet). Let's back up a further step and create a function that processes a set of tokens and returns three components:\n",
    "\n",
    "1. an aggregate sentiment score for the tokens\n",
    "1. a list of positive words\n",
    "1. a list of negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentiment_data_from_tokens(tokens, skipWordNetPos=[]):\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    positives = []\n",
    "    negatives = []\n",
    "    tokens_score = 0\n",
    "    for word, treebank in tagged:\n",
    "        score = get_sentiment_score_from_tagged(word, treebank, skipWordNetPos)\n",
    "        if score:\n",
    "            tokens_score += score\n",
    "            if score > 0:\n",
    "                positives.append(word.lower())\n",
    "            else:\n",
    "                negatives.append(word.lower())\n",
    "    return tokens_score, set(positives), set(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.875, {'good', 'is'}, set())"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentiment_data_from_tokens(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis of Shakespeare's Sonnets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have functions to assign sentiment values and collect positive and negative words for any set of tokens, we can proceed with creating a final function that returns a dictionary with values for each document along with an aggregate list of positive and negative words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentiments_data_from_corpus(corpus, skipWordNetPos=[]):\n",
    "    documents = {}\n",
    "    all_positives = []\n",
    "    all_negatives = []\n",
    "    for fileid in corpus.fileids():\n",
    "        tokens = corpus.words(fileid)\n",
    "        score, positives, negatives = get_sentiment_data_from_tokens(tokens, skipWordNetPos)\n",
    "        documents[fileid] = score\n",
    "        [all_positives.append(positive) for positive in positives]\n",
    "        [all_negatives.append(negative) for negative in negatives]\n",
    "    return documents, set(all_positives), set(all_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now let's pass in our corpus and see what we get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sonnetsSentimentValues, sonnetsPositives, sonnetsNegatives = get_sentiments_data_from_corpus(sonnetsCorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again one of the easiest ways of working with the sentiment values is to create a FreqDist object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sonnetsSentimentFreqs = nltk.FreqDist(sonnetsSentimentValues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see what kind of sentiment values we have, by plotting, for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEoCAYAAAC6v50/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1144335c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "sonnetsSentimentFreqs.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if the x (bottom) axis is illegible, we can see that there are both positive and negatively scored sonnets. Can we determine an average? Sure, let's use numpty.mean() on the list of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0786493506493506"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "numpy.mean([val for doc, val in sonnetsSentimentFreqs.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that on average the sonnets are more positive than negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputting HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to show the most positive and negative sonnets, but to make it even more useful we'd want to show which words are positive and which are negative. Let's create a function to get HTML for any of our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_html_for_sentiment_data(text, positives, negatives):\n",
    "    # the regular expression combines all of the positive and negative words for a search, e.g. (love|like)\n",
    "    # it then surrounds the word found in parentheses with styling, green for positive, red for negative\n",
    "    if len(positives) > 0:\n",
    "        text = re.sub(r'\\b(' + '|'.join(positives) + r')\\b', r'<span style=\"color: green\">\\1</span>', text)\n",
    "    if len(negatives) > 0:\n",
    "        text = re.sub(r'\\b(' + '|'.join(negatives) + r')\\b', r'<span style=\"color: red\">\\1</span>', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to generate an HTML snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileid = sonnetsSentimentFreqs.max() # most positive\n",
    "text = sonnetsCorpus.raw(fileid)\n",
    "html = get_html_for_sentiment_data(text, sonnetsPositives, sonnetsNegatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the HTML above, but it appears as code, so we need to use iPython's facilities for embedding HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>032.txt</h2><pre>  If thou survive my <span style=\"color: green\">well</span>-<span style=\"color: green\">contented</span> day,\r\n",
       "  When that <span style=\"color: red\">churl</span> Death my bones with dust shall cover\r\n",
       "  And shalt by fortune once more re-survey\r\n",
       "  These poor <span style=\"color: green\">rude</span> lines of thy deceased lover,\r\n",
       "  Compare them with the bett'ring of the time,\r\n",
       "  And though they <span style=\"color: green\">be</span> outstripp'<span style=\"color: red\">d</span> by every pen,\r\n",
       "  Reserve them for my <span style=\"color: green\">love</span>, <span style=\"color: red\">not</span> for their rhyme,\r\n",
       "  Exceeded by the height of <span style=\"color: green\">happier</span> men.\r\n",
       "  O! then vouchsafe me but this <span style=\"color: green\">loving</span> <span style=\"color: green\">thought</span>:\r\n",
       "  'Had my <span style=\"color: green\">friend</span>'s Muse grown with this growing age,\r\n",
       "  A dearer birth than this his <span style=\"color: green\">love</span> <span style=\"color: green\">had</span> brought,\r\n",
       "  To march in ranks of <span style=\"color: green\">better</span> equipage:\r\n",
       "    But since he died and <span style=\"color: green\">poets</span> <span style=\"color: green\">better</span> prove,\r\n",
       "    Theirs for their style I'll <span style=\"color: green\">read</span>, his for his <span style=\"color: green\">love</span>'.</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"<h2>\" + fileid + \"</h2><pre>\" + html + \"</pre>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the most negative sonnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>090.txt</h2><pre>  Then <span style=\"color: red\">hate</span> me when thou <span style=\"color: red\">wilt</span>; if ever, now;\r\n",
       "  Now, while the world <span style=\"color: green\">is</span> bent my <span style=\"color: green\">deeds</span> to cross,\r\n",
       "  Join with the <span style=\"color: red\">spite</span> of fortune, <span style=\"color: green\">make</span> me bow,\r\n",
       "  And do <span style=\"color: red\">not</span> drop in for an after-<span style=\"color: red\">loss</span>:\r\n",
       "  Ah! do <span style=\"color: red\">not</span>, when my <span style=\"color: red\">heart</span> hath 'scap'<span style=\"color: red\">d</span> this <span style=\"color: red\">sorrow</span>,\r\n",
       "  Come in the rearward of a conquer'<span style=\"color: red\">d</span> <span style=\"color: red\">woe</span>;\r\n",
       "  Give <span style=\"color: red\">not</span> a windy night a rainy morrow,\r\n",
       "  To linger out a purpos'<span style=\"color: red\">d</span> overthrow.\r\n",
       "  If thou <span style=\"color: red\">wilt</span> leave me, do <span style=\"color: red\">not</span> leave me last,\r\n",
       "  When <span style=\"color: red\">other</span> petty <span style=\"color: red\">griefs</span> <span style=\"color: green\">have</span> done their <span style=\"color: red\">spite</span>,\r\n",
       "  But in the onset come: so shall I taste\r\n",
       "  At first the very <span style=\"color: green\"><span style=\"color: red\">worst</span></span> of fortune's <span style=\"color: red\">might</span>;\r\n",
       "    And <span style=\"color: red\">other</span> strains of <span style=\"color: red\">woe</span>, which now seem <span style=\"color: red\">woe</span>,\r\n",
       "    Compar'<span style=\"color: red\">d</span> with <span style=\"color: red\">loss</span> of thee, <span style=\"color: green\">will</span> <span style=\"color: red\">not</span> seem so.</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileid = sonnetsSentimentFreqs.most_common()[-1][0] # most negative (fileid of the last element in the most common list)\n",
    "text = sonnetsCorpus.raw(fileid)\n",
    "html = get_html_for_sentiment_data(text, sonnetsPositives, sonnetsNegatives)\n",
    "HTML(\"<h2>\" + fileid + \"</h2><pre>\" + html + \"</pre>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot that we could look at and question here, and we should do so, but there's also some intriguing aspects, particularly for such an automated process.\n",
    "\n",
    "For the sake of convenience, here are the main functions that we defined for sentiment analysis of a corpus:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "def get_sentiments_data_from_corpus(corpus, skipWordNetPos=[]):\n",
    "    documents = {}\n",
    "    all_positives = []\n",
    "    all_negatives = []\n",
    "    for fileid in corpus.fileids():\n",
    "        tokens = corpus.words(fileid)\n",
    "        score, positives, negatives = get_sentiment_data_from_tokens(tokens, skipWordNetPos)\n",
    "        documents[fileid] = score\n",
    "        [all_positives.append(positive) for positive in positives]\n",
    "        [all_negatives.append(negative) for negative in negatives]\n",
    "    return documents, set(all_positives), set(all_negatives)\n",
    "\n",
    "def get_sentiment_data_from_tokens(tokens, skipWordNetPos=[]):\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    positives = []\n",
    "    negatives = []\n",
    "    tokens_score = 0\n",
    "    for word, treebank in tagged:\n",
    "        score = get_sentiment_score_from_tagged(word, treebank, skipWordNetPos)\n",
    "        if score:\n",
    "            tokens_score += score\n",
    "            if score > 0:\n",
    "                positives.append(word.lower())\n",
    "            else:\n",
    "                negatives.append(word.lower())\n",
    "    return tokens_score, set(positives), set(negatives)\n",
    "\n",
    "def get_sentiment_score_from_tagged(token, treebank, skipWordNetPos=[]):\n",
    "    wordnet_pos = treebank_to_wordnet_pos(treebank, skipWordNetPos)\n",
    "    if wordnet_pos: # only print matches\n",
    "        senti_synsets = list(swn.senti_synsets(token, wordnet_pos))\n",
    "        if senti_synsets:\n",
    "            return senti_synsets[0].pos_score() - senti_synsets[0].neg_score()\n",
    "\n",
    "def treebank_to_wordnet_pos(treebank, skipWordNetPos=[]):\n",
    "    if \"NN\" in treebank and \"n\" not in skipWordNetPos: # singular and plural nouns (NN, NNS)\n",
    "        return \"n\"\n",
    "    elif \"JJ\" in treebank and \"a\" not in skipWordNetPos: # adjectives (JJ, JJR, JJS)\n",
    "        return \"a\" \n",
    "    elif \"VB\" in treebank and \"v\" not in skipWordNetPos: # verbs (VB, VBD, VBG, VBN, VBP, VBZ)\n",
    "        return \"v\"\n",
    "    elif \"RB\" in treebank and \"r\" not in skipWordNetPos: # adverbs (RB, RBR, RBS)\n",
    "        return \"r\"\n",
    "    # if we don't match any of these we implicitly return None```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try these tasks:\n",
    "\n",
    "* Experiment with different values for *skipWordNetPos* – which combination seems to give the best results?\n",
    "* Can you set a threshold of positive and negative values to improve results?\n",
    "* Can you add your own stopword list to the function signatures and have those words skipped when looking for sentiment?\n",
    "\n",
    "In the next notebook we're going to look at [Document Similarity](DocumentSimilarity.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "[CC BY-SA](https://creativecommons.org/licenses/by-sa/4.0/) From [The Art of Literary Text Analysis](ArtOfLiteraryTextAnalysis.ipynb) by [Stéfan Sinclair](http://stefansinclair.name) &amp; [Geoffrey Rockwell](http://geoffreyrockwell.com)<br >Created March 9, 2015 and last modified December 9, 2015 (Jupyter 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
